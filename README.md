# Hypertune Segmentation Networks with PyTorch\n\nThis repository provides a flexible framework for hyperparameter tuning of semantic segmentation networks. It combines segmentation models, adaptive augmentation, and hyperparameter search in one streamlined pipeline.\n\n---\n\n## 🚀 Features\n- Segmentation Networks: Based on segmentation_models.pytorch (SMP) \n- Hyperparameter Optimization: Powered by Optuna for efficient search strategies (TPE, grid, random search, pruning) \n- Tunable Adaptive Augmentation: Inspired by Hou et al. (Monotonic Curriculum, 2023) – progressively introduces stronger augmentations as training progresses \n- PyTorch Lightning integration for clean training loops and logging \n- Config-driven design: Define search spaces, training params, and augmentations via YAML configs \n- Debug-friendly dataset class with adaptive augmentation hooks \n\n---\n\n## 📂 Repository Structure\n\n├── configs/ # Example configs for training & tuning\n│ └── default.yaml\n├── src/\n│ ├── data/ # Dataset & DataModule definitions\n│ │ └── dataset.py\n│ ├── models/ # Model initialization from SMP\n│ ├── train.py # Standard training script\n│ └── tune.py # Optuna-based hyperparameter search\n├── .gitignore\n├── LICENSE\n└── README.md\n\n\n---\n\n## ⚙️ Installation\n\nbash\n# Clone repo\ngit clone https://github.com/<your-username>/<repo-name>.git\ncd <repo-name>\n\n# Create environment\nconda create -n segm python=3.10\nconda activate segm\n\n# Install dependencies\npip install -r requirements.txt\n\n\nKey Dependencies \n- PyTorch \n- PyTorch Lightning \n- segmentation_models.pytorch \n- Optuna \n- Albumentations \n\n---\n\n## 🧑‍💻 Usage\n\n### Train a model\nbash\npython src/train.py --config configs/default.yaml\n\n\n### Run hyperparameter search\nbash\npython src/tune.py --config configs/default.yaml --trials 50\n\n\n### Configurable parameters\n- Encoder architecture (resnet34, efficientnet-b0, …) \n- Decoder type (Unet, FPN, DeepLabV3+) \n- Loss functions (DiceLoss, CrossEntropy, Combo) \n- Learning rate, optimizer, scheduler \n- Augmentation strategy (static vs. adaptive curriculum) \n\n---\n\n## 📖 Adaptive Augmentation (Hou et al., 2023)\n\nThis repo implements an epoch-dependent curriculum augmentation, following:\n\n> Monotonic curriculum which progressively introduces more augmented samples as the training epoch increases. \n> — Hou et al., Paper\n\n- At early epochs → lighter augmentations \n- At later epochs → stronger, more challenging augmentations \n- Controlled by tau parameter in the config \n\n---\n\n## 📊 Example Workflow\n\n1. Define dataset paths in configs/default.yaml \n2. Choose encoder/decoder and loss function \n3. Run train.py for a baseline model \n4. Run tune.py to explore hyperparameters \n5. Monitor results via Optuna dashboard:\n bash\n optuna-dashboard sqlite:///optuna.db\n \n\n---\n\n## 📜 License\nThis project is licensed under the MIT License.\n\n---\n\n## 🙌 Acknowledgements\n- segmentation_models.pytorch by Pavel Yakubovskiy \n- Optuna for hyperparameter optimization \n- Albumentations for data augmentation \n- Hou et al., 2023: Inspiration for adaptive augmentation curriculum \n```"}
